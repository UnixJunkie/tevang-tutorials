{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced in silico drug design workshop. Olomouc, 21-25 January, 2019.\n",
    "### Deep Leaning Tutorial: Multi-Layer Perceptron with Keras\n",
    "\n",
    "Dr Thomas Evangelidis\n",
    "\n",
    "Research Scientist\n",
    "\n",
    "IOCB - Institute of Organic Chemistry and Biochemistry of the Czech Academy of Sciences\n",
    "Prague, Czech Republic\n",
    "\n",
    "  & \n",
    "CEITEC - Central European Institute of Technology\n",
    "Brno, Czech Republic \n",
    "\n",
    "email: tevang3@gmail.com\n",
    "\n",
    "website: https://sites.google.com/site/thomasevangelidishomepage/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this tutorial you will learn how to construct a simple Multi-Layer Perceptron model with Keras. Specifically you will learn to:\n",
    "* Create and add layers including weight initialization and activation.\n",
    "* Compile models including optimization method, loss function and metrics.\n",
    "* Fit models include epochs and batch size.\n",
    "* Model predictions.\n",
    "* Summarize the model.\n",
    "* Train an initial model on large relevant data and transfer the hidden layers of that model to a new one, which will be training with fewer focused data (Transfer LEarning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras.callbacks import EarlyStopping\n",
    "from rdkit.Chem import AllChem, SDMolSupplier\n",
    "from rdkit import DataStructs\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Dense\n",
    "from scipy.stats import kendalltau, pearsonr\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.model_selection import cross_validate, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading molecules and activity from SDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"data/cdk2.sdf\"\n",
    "\n",
    "mols = []\n",
    "y = []\n",
    "for mol in SDMolSupplier(fname):\n",
    "    if mol is not None:\n",
    "        mols.append(mol)\n",
    "        y.append(float(mol.GetProp(\"pIC50\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate descriptors (fingerprints) and convert them into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate binary Morgan fingerprint with radius 2\n",
    "fp = [AllChem.GetMorganFingerprintAsBitVect(m, 2) for m in mols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdkit_numpy_convert(fp):\n",
    "    output = []\n",
    "    for f in fp:\n",
    "        arr = np.zeros((1,))\n",
    "        DataStructs.ConvertToNumpyArray(f, arr)\n",
    "        output.append(arr)\n",
    "    return np.asarray(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rdkit_numpy_convert(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 2019\n",
    "np.random.seed(seed)\n",
    "mol_num, feat_num = x.shape\n",
    "print(\"# molecules for training = %i, # of features = %i\\n\" % (mol_num, feat_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to create a simple feed forward network with one fully connected hidden layer or 300 neurons. The network uses the rectifier activation function for the hidden layer. No activation function is used for the output layer because it is a regression problem and we are interested in predicting numerical values directly without transform. The ADAM algorithm is employed to optimize the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "def MLP1(feat_num, loss):\n",
    "    net = Sequential()\n",
    "    net.add(Dense(300, input_dim=feat_num, kernel_initializer='normal', activation='relu'))\n",
    "    net.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    net.compile(loss=loss, optimizer='adam')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print summary of layers and trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_208 (Dense)            (None, 300)               614700    \n_________________________________________________________________\ndense_209 (Dense)            (None, 1)                 301       \n=================================================================\nTotal params: 615,001\nTrainable params: 615,001\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MLP1(feat_num, 'mean_squared_error').summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model with Keras wrapper of scikit-learn (faster and easier), using Mean Squared Error as the loss function, 300 training epochs and batch size 1/8 of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=MLP1, \n",
    "                           feat_num=feat_num, \n",
    "                           loss='mean_squared_error', \n",
    "                           epochs=300, \n",
    "                           batch_size=int(x.shape[0]/8), \n",
    "                           verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define our own evaluation metrics for the model: 1) Kendall's tau (ranking correlation), 2) Pearson's R (correlation), 3) Mean Squared Error. The evaluation will be done with 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score_time': array([0.72287917, 0.73120093, 0.73302698, 0.74567223, 0.74707484]), 'test_MSE': array([1.23098004, 1.85036503, 1.34946516, 0.74074917, 1.04380864]), 'fit_time': array([17.85927796, 18.00152087, 17.98357105, 17.95444679, 17.97064114]), 'test_R': array([0.61573605, 0.48036024, 0.71477564, 0.84383886, 0.83615917]), 'test_tau': array([0.40837709, 0.29654438, 0.37620499, 0.60010937, 0.5901813 ])}\n\nResults: average tau=0.454283+-0.120680, average R=0.698174+-0.137675, average MSE=1.243074+-0.366689\n\n"
     ]
    }
   ],
   "source": [
    "def kendalls_tau(estimator, X, y):\n",
    "    preds = estimator.predict(X)\n",
    "    t = kendalltau(preds, y)[0]\n",
    "    return t\n",
    "\n",
    "def pearsons_r(estimator, X, y):\n",
    "    preds = estimator.predict(X)\n",
    "    r = pearsonr(preds, y)[0]\n",
    "    return r\n",
    "\n",
    "def MSE(estimator, X, y):\n",
    "    preds = estimator.predict(X)\n",
    "    mse = mean_squared_error(preds, y)\n",
    "    return mse\n",
    "    \n",
    "    \n",
    "scoring = {'tau': kendalls_tau, 'R':pearsons_r, 'MSE':MSE}\n",
    "kfold = KFold(n_splits=5, random_state=seed)\n",
    "scores = cross_validate(estimator, x, y, scoring=scoring, cv=kfold, return_train_score=False)\n",
    "print(scores)\n",
    "print(\"\\nResults: average tau=%f+-%f, average R=%f+-%f, average MSE=%f+-%f\\n\" % \n",
    "      (scores['test_tau'].mean(), scores['test_tau'].std(),\n",
    "       scores['test_R'].mean(), scores['test_R'].std(),\n",
    "       scores['test_MSE'].mean(), scores['test_MSE'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running this code gives us an estimate of the modelâ€™s performance on the problem for unseen data. The result reports the average and standard deviation of each metric across all 5 folds of the cross validation evaluation.\n",
    "\n",
    "#### Now let's try Absolute Mean Error as a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=MLP1, \n",
    "                           feat_num=feat_num, \n",
    "                           loss='mean_absolute_error', \n",
    "                           epochs=300, \n",
    "                           batch_size=int(x.shape[0]/8),\n",
    "                           verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score_time': array([0.56212401, 0.57379794, 0.57216287, 0.58115292, 0.57715392]), 'test_MSE': array([1.41889084, 1.6827523 , 2.1521906 , 0.87549575, 0.86927765]), 'fit_time': array([17.00753903, 17.0233171 , 17.04589701, 17.09628987, 16.98412704]), 'test_R': array([0.60938719, 0.53293132, 0.62721648, 0.82024583, 0.86493694]), 'test_tau': array([0.40209436, 0.33190468, 0.33700571, 0.60975829, 0.63473341])}\n\nResults: average tau=0.463099+-0.132514, average R=0.690944+-0.128587, average MSE=1.399721+-0.490483\n\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(estimator, x, y, scoring=scoring, cv=kfold, return_train_score=False)\n",
    "print (scores)\n",
    "print(\"\\nResults: average tau=%f+-%f, average R=%f+-%f, average MSE=%f+-%f\\n\" % \n",
    "      (scores['test_tau'].mean(), scores['test_tau'].std(),\n",
    "       scores['test_R'].mean(), scores['test_R'].std(),\n",
    "       scores['test_MSE'].mean(), scores['test_MSE'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see a subtle performance increase.\n",
    "\n",
    "#### Now let's add an extra hidden layer to the network with 200 neurons, to see if the performance improves further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "def MLP2(feat_num, loss):\n",
    "    net = Sequential()\n",
    "    net.add(Dense(300, input_dim=feat_num, kernel_initializer='normal', activation='relu'))\n",
    "    net.add(Dense(200, input_dim=feat_num, kernel_initializer='normal', activation='relu'))\n",
    "    net.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    net.compile(loss=loss, optimizer='adam')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print summary of layers and trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_230 (Dense)            (None, 300)               614700    \n_________________________________________________________________\ndense_231 (Dense)            (None, 200)               60200     \n_________________________________________________________________\ndense_232 (Dense)            (None, 1)                 201       \n=================================================================\nTotal params: 675,101\nTrainable params: 675,101\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MLP2(feat_num, 'mean_absolute_error').summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We increase the training epochs to 500 because the addition of an extra layer increased the trainable variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=MLP2, \n",
    "                           feat_num=feat_num, \n",
    "                           loss='mean_absolute_error', \n",
    "                           epochs=500,\n",
    "                           batch_size=int(x.shape[0]/8), \n",
    "                           verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score_time': array([0.59869599, 0.591995  , 0.60312104, 0.60489297, 0.61515594]), 'test_MSE': array([1.27065902, 1.69945694, 2.60388686, 0.85742932, 0.85854965]), 'fit_time': array([29.43800306, 29.42567801, 29.50881195, 29.47552991, 29.63292408]), 'test_R': array([0.61633608, 0.55587068, 0.51790747, 0.82425183, 0.86172713]), 'test_tau': array([0.40575929, 0.36244313, 0.25657722, 0.59957331, 0.62077733])}\n\nResults: average tau=0.449026+-0.140405, average R=0.675219+-0.141035, average MSE=1.457996+-0.652147\n\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(estimator, x, y, scoring=scoring, cv=kfold, return_train_score=False)\n",
    "print scores\n",
    "print(\"\\nResults: average tau=%f+-%f, average R=%f+-%f, average MSE=%f+-%f\\n\" % \n",
    "      (scores['test_tau'].mean(), scores['test_tau'].std(),\n",
    "       scores['test_R'].mean(), scores['test_R'].std(),\n",
    "       scores['test_MSE'].mean(), scores['test_MSE'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We don't see statistically significant improvement because our training set is small (436 molecules)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use all binding assays for CDK2 from CHEMBL (1519 molecules) to train a network and then we will transfer its hidden layer to a new network which we shall train with the smaller training set that we have been using so far.\n",
    "\n",
    "#### The following block of code is just for data preparation,  you can go hrough it very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TRAINING DATA FOR TRANSFER LEARNING\n",
    "fname = \"data/cdk2_large.sdf\"\n",
    "\n",
    "mols = []\n",
    "molnames = []\n",
    "for mol in SDMolSupplier(fname):\n",
    "    if mol is not None:\n",
    "        molname = mol.GetProp(\"_Name\")\n",
    "        if not molname in molnames:\n",
    "            molnames.append(molname)\n",
    "            mols.append(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "molname2pK_dict = {}\n",
    "with open(\"data/cdk2_pK.dat\", 'r') as f:\n",
    "    for line in f:\n",
    "        molname, pK = line.split()\n",
    "        if not molname in molname2pK_dict.keys():\n",
    "            molname2pK_dict[molname] = float(pK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "molnames1 = set(molnames)\n",
    "molnames2 = set(molname2pK_dict.keys())\n",
    "common_molnames = molnames1.intersection(molnames2)\n",
    "y_transf = [molname2pK_dict[molname] for molname in molnames if molname in common_molnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate binary Morgan fingerprint with radius 2 as feature vectors for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = [AllChem.GetMorganFingerprintAsBitVect(m, 2) for m in mols if m.GetProp(\"_Name\") in common_molnames]\n",
    "x_transf = rdkit_numpy_convert(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# molecules for transfer training = 1519, # of features = 2048\n\n"
     ]
    }
   ],
   "source": [
    "mol_num, feat_num = x_transf.shape\n",
    "print(\"# molecules for transfer training = %i, # of features = %i\\n\" % (mol_num, feat_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We train a network with one fully connected hidden layer of 300 neurons, like in our first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b424b831990>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP1(feat_num=feat_num, \n",
    "           loss='mean_absolute_error')\n",
    "net.fit(x_transf, \n",
    "        y_transf,\n",
    "        epochs=300, \n",
    "        batch_size=int(x_transf.shape[0]/8), \n",
    "        verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below we create a function that transfers hiddel layers (up to index 'idx', starting from 0) to a new network. 'lhl_sizes' is a tuple defining the number of neurons in each hidden layer, e.g. (200,100) means two hidden layers with 200 and 100 neurons respectivelly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transf_MLP(feat_num, idx, lhl_sizes, loss='mean_absolute_error'):\n",
    "    global net  # net is a networks and cannot be pickled! Therefore it cannot be an input argument for cross_validate() to work!\n",
    "    inp = Input(shape=(feat_num,))\n",
    "    shared_layer = net.layers[0]\n",
    "    shared_layer.trainable = False  # deactivate training in all re-used layers of MLP1\n",
    "    out_tensor = shared_layer(inp)\n",
    "    # idx = 1  # index of desired layer\n",
    "    for i in range(1,idx+1):\n",
    "        shared_layer = net.layers[i]    # deactivate training in all re-used layers of MLP1\n",
    "        shared_layer.trainable = False  # deactivate training in all re-used layers of MLP1\n",
    "        out_tensor = shared_layer(out_tensor)\n",
    "    # Here add all the new layers\n",
    "    for l_size in lhl_sizes:\n",
    "        out_tensor = Dense(l_size, kernel_initializer='normal', activation='relu')(out_tensor)\n",
    "    # Close the network\n",
    "    out_tensor = Dense(1, kernel_initializer='normal')(out_tensor)\n",
    "    # Create the model\n",
    "    transf_model = Model(inp, out_tensor)\n",
    "    transf_model.compile(loss=loss, optimizer='adam')\n",
    "    return transf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=transf_MLP,\n",
    "                           feat_num=feat_num,\n",
    "                           idx=0,\n",
    "                           lhl_sizes=(300, 200),\n",
    "                           loss='mean_absolute_error',\n",
    "                           epochs=300,\n",
    "                           batch_size=int(x.shape[0]/8),\n",
    "                           verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure the performance of the new hybrid network on our initial small dataset with 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score_time': array([0.61949897, 0.6279192 , 0.63407707, 0.63969994, 0.65007305]), 'test_MSE': array([1.32864658, 1.03584473, 0.80954335, 0.61161483, 1.02622082]), 'fit_time': array([ 9.79497313,  9.34225893, 24.54360104,  9.45413208,  9.42776895]), 'test_R': array([0.57447738, 0.80218974, 0.81670668, 0.88901651, 0.86815936]), 'test_tau': array([0.34607341, 0.62068051, 0.6164606 , 0.71482433, 0.66855008])}\n\nResults: average tau=0.593318+-0.128715, average R=0.790110+-0.112450, average MSE=0.962374+-0.240840\n\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(estimator, x, y, scoring=scoring, cv=kfold, return_train_score=False)\n",
    "print (scores)\n",
    "print(\"\\nResults: average tau=%f+-%f, average R=%f+-%f, average MSE=%f+-%f\\n\" %\n",
    "      (scores['test_tau'].mean(), scores['test_tau'].std(),\n",
    "       scores['test_R'].mean(), scores['test_R'].std(),\n",
    "       scores['test_MSE'].mean(), scores['test_MSE'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see an impressive performance gain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score_time': array([0.35903096, 0.35371709, 0.36846399, 0.36452699, 0.37568092]), 'test_MSE': array([1.16594237, 1.34469876, 0.78113445, 1.14604242, 1.09889925]), 'fit_time': array([9.7914331 , 7.95971394, 8.05258393, 8.59206796, 8.59934092]), 'test_R': array([0.69472628, 0.79244967, 0.8679898 , 0.81822004, 0.8420003 ]), 'test_tau': array([0.50000015, 0.60782221, 0.61140077, 0.65371449, 0.59554902])}\n\nResults: average tau=0.593697+-0.050789, average R=0.803077+-0.059684, average MSE=1.107343+-0.183168\n\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(estimator, x, y, scoring=scoring, cv=kfold, return_train_score=False,)\n",
    "print scores\n",
    "print(\"\\nResults: average tau=%f+-%f, average R=%f+-%f, average MSE=%f+-%f\\n\" %\n",
    "      (scores['test_tau'].mean(), scores['test_tau'].std(),\n",
    "       scores['test_R'].mean(), scores['test_R'].std(),\n",
    "       scores['test_MSE'].mean(), scores['test_MSE'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Transfer learning does not always improve the performance. In this case we used a larger set of compounds binding to the same receptor (CDK2) to pretrain an network and transfer it's hidden layer to another one. If we have done the same but with compounds binding to CDK1 (59% sequence identity with CDK2) then the performance would have been worse. So be caucious where you apply transfer learning and which training data you use!\n",
    "As a home work you can apply the same procedure by instead of \"data/cdk2_large.sdf\" and \"data/cdk2_pK.dat\", to use \"data/cdk1_large.sdf\" and \"data/cdk1_pK.dat\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Classifier network instead of a Regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The difference with our Regressor MLP1 is that the output layer contains a single neuron and uses the sigmoid activation function in order to produce a probability output in the range of 0 to 1 that can easily and automatically be converted to class values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Classifier\n",
    "def MLP3(feat_num, loss='binary_crossentropy'):\n",
    "    net = Sequential()\n",
    "    net.add(Dense(300, input_dim=feat_num, kernel_initializer='normal', activation='relu'))\n",
    "    net.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    net.compile(optimizer='rmsprop', loss=loss, metrics=['accuracy'])\n",
    "    # Compile model\n",
    "    net.compile(loss=loss, optimizer='adam')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and prepare the Blood Brain Barrier permeability data for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"data/logBB.sdf\"\n",
    "\n",
    "mols = []\n",
    "y = []\n",
    "for mol in SDMolSupplier(fname):\n",
    "    if mol is not None:\n",
    "        mols.append(mol)\n",
    "        y.append(float(mol.GetProp(\"logBB_class\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate binary Morgan fingerprint with radius 2 for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = [AllChem.GetMorganFingerprintAsBitVect(m, 2) for m in mols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rdkit_numpy_convert(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# molecules for training = 321, # of features = 2048\n\n"
     ]
    }
   ],
   "source": [
    "mol_num, feat_num = x.shape\n",
    "print(\"# molecules for training = %i, # of features = %i\\n\" % (mol_num, feat_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print summary of layers and trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_267 (Dense)            (None, 300)               614700    \n_________________________________________________________________\ndense_268 (Dense)            (None, 1)                 301       \n=================================================================\nTotal params: 615,001\nTrainable params: 615,001\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MLP3(feat_num, 'binary_crossentropy').summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=MLP3, \n",
    "                           feat_num=feat_num, \n",
    "                           loss='binary_crossentropy', \n",
    "                           epochs=100,  # ~300 is the optimum value, but we use 1000 to see the effect of overfitting\n",
    "                           batch_size=int(x.shape[0]/8), \n",
    "                           verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this time a classification metric to score the predictions, the area under the Receiver Operating Characteristic Curve (AUC-ROC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score_time': array([0.68351793, 0.65033007, 0.65935397, 0.66439486, 0.6893599 ]), 'test_roc': array([0.71388889, 0.66966967, 0.79545455, 0.78615385, 0.61862745]), 'fit_time': array([6.90146089, 6.71428204, 6.75946999, 6.74006104, 6.746907  ])}\n\nResults: average AUC-ROC=0.716759+-0.067623\n\n"
     ]
    }
   ],
   "source": [
    "def AUC_ROC(estimator, X, y):\n",
    "    preds = estimator.predict(X)\n",
    "    auc = roc_auc_score(preds, y)\n",
    "    return auc\n",
    "\n",
    "scoring = {'roc': AUC_ROC}\n",
    "kfold = KFold(n_splits=5, random_state=seed)\n",
    "scores = cross_validate(estimator, x, y, scoring=scoring, cv=kfold, return_train_score=False)\n",
    "print (scores)\n",
    "print(\"\\nResults: average AUC-ROC=%f+-%f\\n\" % \n",
    "      (scores['test_roc'].mean(), scores['test_roc'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The MLP classifier had a relative good performance. Compare with with ML model performance from the QSAR tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use early stopping to see if the performance improves even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score_time': array([0.68597102, 0.68706489, 0.70336699, 0.69680381, 0.71951294]), 'test_roc': array([0.67917548, 0.66966967, 0.81118494, 0.79757085, 0.61862745]), 'fit_time': array([7.01429892, 6.82169008, 6.8170321 , 6.82987309, 6.84853411])}\n\nResults: average AUC-ROC=0.715246+-0.075756\n\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(estimator, x, y, scoring=scoring, cv=kfold, return_train_score=False,\n",
    "                        fit_params={'callbacks': [EarlyStopping(patience=3)]})\n",
    "print (scores)\n",
    "print(\"\\nResults: average AUC-ROC=%f+-%f\\n\" % \n",
    "      (scores['test_roc'].mean(), scores['test_roc'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to recreate each of the network architectures that you created with Keras using the following online tool:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### http://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We don't see statistically significant change, probably because the training set is small (321 molecules). In real case problems you have thousands of training samples and there the effects of overfitting are more evident."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}